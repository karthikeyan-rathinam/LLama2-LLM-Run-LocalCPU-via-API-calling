from http.server import BaseHTTPRequestHandler, HTTPServer
from urllib.parse import urlparse, parse_qs
from langchain.llms import CTransformers
from langchain import PromptTemplate, LLMChain
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import json
import logging

# Configure logging
logging.basicConfig(
    filename='llama_General_Chabot.log',  # You can adjust the log file name here
    filemode='a',
    format='[%(asctime)s] [%(levelname)s] [%(filename)s] [%(lineno)s:%(funcName)s()] %(message)s',
    datefmt='%Y-%b-%d %H:%M:%S'
)
LOGGER = logging.getLogger(__name__)

log_level_env = 'INFO'  # You can adjust the log level here
log_level_dict = {
    'DEBUG': logging.DEBUG,
    'INFO': logging.INFO,
    'WARNING': logging.WARNING,
    'ERROR': logging.ERROR,
    'CRITICAL': logging.CRITICAL
}
if log_level_env in log_level_dict:
    log_level = log_level_dict[log_level_env]
else:
    log_level = log_level_dict['INFO']
LOGGER.setLevel(log_level)

llm=""
class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):
    def _set_headers(self, content_type='text/html', status=200):

        """
        Set the HTTP headers for the response.

        Parameters:
        - content_type (str): The content type of the response.
        - status (int): The HTTP status code.
        """
        try:
          self.send_response(status)
          self.send_header('Content-type', content_type)
          self.end_headers()
        except Exception as e:
            return str(e)

    def model_response(self, question):

        """
        Get the response from the language model based on the provided question.

        Parameters:
        - question (str): The input question for the language model.

        Returns:
        - str: The response generated by the language model.
        """

        try:
          LOGGER.info("Getting Model Response...")
          global llm

          # Check if the language model is not initialized
          if llm == "":

              # Initialize the CTransformers language mode
              llm = CTransformers(model="TheBloke/Llama-2-7B-Chat-GGML", model_file = 'llama-2-7b-chat.ggmlv3.q2_K.bin', callbacks=[StreamingStdOutCallbackHandler()])

          # Template for language model input
          template = """
          [INST] <<SYS>>
          You are a helpful, respectful and honest assistant. Your answers are always brief.
          <</SYS>>
          {text}[/INST]
          """

          # Create a prompt template with input variables
          prompt = PromptTemplate(template=template, input_variables=["text"])

          # Create an LLMChain with the defined prompt and language model
          llm_chain = LLMChain(prompt=prompt, llm=llm)

          # Generate a response using the LLMChain
          response = llm_chain.run(question)

          return response

        except Exception as e:
            return str(e)
    
    def do_GET(self):

        """
        Handle GET requests by extracting the question from the query parameters
        and responding with the generated model response in JSON format.
        """
        try:
          parsed_url = urlparse(self.path)
          query_params = parse_qs(parsed_url.query)
          
          # Get the question from query parameters
          received_question = query_params.get('question', [''])[0]

          # Generate model response
          model_response=self.model_response(received_question)

          # Prepare JSON response
          response_data = {'response': f"{model_response}"}
          response = json.dumps(response_data).encode('utf-8')

          # Set headers and write the response
          self._set_headers('application/json')
          self.wfile.write(response)
        except Exception as e:
            return str(e)

def run(server_class=HTTPServer, handler_class=SimpleHTTPRequestHandler, port=8000):

    """
    Run the HTTP server with the specified server class, request handler class, and port.

    Parameters:
    - server_class: The HTTP server class.
    - handler_class: The request handler class.
    - port: The port on which the server will listen.
    """
    try:
      server_address = ('', port)
      httpd = server_class(server_address, handler_class)
      print(f"Server running on port {port}")
      httpd.serve_forever()
    except Exception as e:
            return str(e)

if __name__ == '__main__':
    run()